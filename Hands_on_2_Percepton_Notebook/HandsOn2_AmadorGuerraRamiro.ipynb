{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Hands-on 2: kNN Notebook (Tutorial) Ramiro Amador Guerra"
      ],
      "metadata": {
        "id": "LdfGVLTLbinT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Fundamentos de la técnica kNN\n",
        "\n",
        "La técnica de **k-Nearest Neighbors (kNN)** es un método de clasificación supervisada que asigna una clase a un nuevo patrón basado en las clases de sus **k vecinos más cercanos** en el espacio de características.  \n",
        "\n",
        "## Características principales:\n",
        "- No requiere entrenamiento explícito (algoritmo perezoso).\n",
        "- Basado en medidas de distancia, normalmente Euclidiana.\n",
        "- Sensible a la escala de las variables.\n"
      ],
      "metadata": {
        "id": "bFH5uuSZdoGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Modelo Matemático\n",
        "\n",
        "Sea un conjunto de datos de entrenamiento:\n",
        "$$\n",
        "\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}\n",
        "$$\n",
        "\n",
        "Donde $x_i \\in \\mathbb{R}^m$ es un vector de características y $y_i$ la clase.\n",
        "\n",
        "Para un nuevo punto $x$:\n",
        "\n",
        "1.  Calcular la distancia (usualmente **Euclidiana**) entre $x$ y todos los puntos del conjunto de entrenamiento:\n",
        "$$\n",
        "d(x, x_i) = \\sqrt{\\sum_{j=1}^{m} (x_j - x_{ij})^2}\n",
        "$$\n",
        "\n",
        "2.  Seleccionar los $k$ vecinos más cercanos.\n",
        "\n",
        "3.  Asignar la clase mayoritaria entre esos vecinos (función **Moda**):\n",
        "$$\n",
        "y = \\text{modo}\\{y_i \\text{ de los k vecinos más cercanos}\\}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "O-RmK-G5d3ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Librerías y Funciones\n",
        "\n",
        "## Librerías principales:\n",
        "- **numpy**: manipulación de arrays.\n",
        "- **pandas**: manejo de datasets.\n",
        "- **matplotlib / seaborn**: visualización de datos.\n",
        "- **scikit-learn**: implementación de kNN y métricas de evaluación.\n",
        "\n",
        "## Clases y funciones de scikit-learn:\n",
        "- `KNeighborsClassifier`: modelo kNN.\n",
        "- `train_test_split`: dividir dataset en entrenamiento y prueba.\n",
        "- `StandardScaler`: estandarización de datos.\n",
        "- `confusion_matrix`, `accuracy_score`: métricas de evaluación.\n"
      ],
      "metadata": {
        "id": "9eUZIvfPd6_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Preprocesamiento\n",
        "# ---------------------\n",
        "\n",
        "# Importar librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Cargar dataset de ejemplo (Iris)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target, name='target')\n",
        "\n",
        "# Estadísticas básicas\n",
        "print(\"Resumen estadístico del dataset:\")\n",
        "display(X.describe())\n",
        "\n",
        "# Gráfico de dispersión\n",
        "sns.pairplot(pd.concat([X, y.rename('target')], axis=1), hue='target')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1rbvRfa-d9kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Feature Engineering\n",
        "\n",
        "Seleccionaremos todas las variables del dataset (`sepal length`, `sepal width`, `petal length`, `petal width`) como características para entrenar el modelo, ya que todas son relevantes para la clasificación de la especie de Iris.  \n",
        "\n",
        "En casos reales, se puede usar análisis de correlación o selección de características.\n"
      ],
      "metadata": {
        "id": "LyhapeNcd_fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.3 Predicción\n",
        "# ---------------------\n",
        "\n",
        "# División de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Estandarización de datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Entrenamiento del modelo kNN\n",
        "k = 5\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Función para predecir clase de un nuevo patrón\n",
        "def predecir_nuevo_patron(patron):\n",
        "    patron_scaled = scaler.transform([patron])\n",
        "    clase = knn.predict(patron_scaled)\n",
        "    print(f\"El patrón {patron} se clasifica como clase: {iris.target_names[clase][0]}\")\n",
        "\n",
        "# Ejemplo de predicción\n",
        "predecir_nuevo_patron([5.1, 3.5, 1.4, 0.2])\n"
      ],
      "metadata": {
        "id": "YlkydK_keB_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.4 Evaluación del Modelo\n",
        "# ---------------------\n",
        "\n",
        "# Predicciones sobre el conjunto de prueba\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Matriz de Confusión\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de Confusión:\\n\", cm)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Precisión del modelo: {acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "hIPTot2MeDiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretación de resultados\n",
        "\n",
        "- La **matriz de confusión** permite ver cuántas muestras de cada clase fueron clasificadas correctamente o incorrectamente.\n",
        "- La **precisión (accuracy)** indica el porcentaje de predicciones correctas sobre el total de muestras.\n"
      ],
      "metadata": {
        "id": "t-eFlQQceEtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bibliografia\n",
        "\n",
        "- Díaz, R. (2024, 6 julio). Algoritmo KNN – cómo funciona y ejemplos en Python. The Machine Learners. https://www.themachinelearners.com/algoritmo-knn/\n",
        "\n",
        "- GeeksforGeeks. (2025, 23 agosto). KNeArest Neighbor(KNN) algorithm. GeeksforGeeks. https://www.geeksforgeeks.org/machine-learning/\n",
        "k-nearest-neighbours/ Korstanje, J. (2022, 1 septiembre).\n",
        "\n",
        "- The k-Nearest Neighbors (kNN) Algorithm in Python. https://realpython.com/knn-python/"
      ],
      "metadata": {
        "id": "55QbBCTUeH8n"
      }
    }
  ]
}